{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualisation of the passenger punctuality results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition of functions for reading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "##  Reading traffic data\n",
    "#######################\n",
    "def get_traffic_data():\n",
    "    input_file = 'C:/Users/AbdouAA/Work Folders/Documents/GitHub/k2-respunkt/data/RST_2015_v38_42.csv'\n",
    "    traffic_data = pd.read_csv(input_file, delimiter=';')\n",
    "    return traffic_data\n",
    "\n",
    "\n",
    "######################\n",
    "##  Reading demand data\n",
    "#######################\n",
    "def get_demand_data():\n",
    "    # load dataset\n",
    "    input_file = 'C:/Users/AbdouAA/Work Folders/Documents/GitHub/k2-respunkt/data/OD_data_dynamic.xlsx'\n",
    "    df_static = pd.read_excel(input_file, sheet_name='Static', index_col=0, header=0)\n",
    "\n",
    "    # Extract headers and index from the static data\n",
    "    headers = df_static.columns\n",
    "    index = df_static.index\n",
    "\n",
    "    # Number of time periods (every 15 minutes during a full day)\n",
    "    nb_time_periods = int(24 * 60 / 15)  # 96 periods\n",
    "\n",
    "    # Initialize a dictionary to store DataFrames for each time period\n",
    "    df_sheets = {}\n",
    "\n",
    "    # Read specific sheets by index\n",
    "    for t in range(nb_time_periods):\n",
    "        sheet_name = f\"Sheet{t+1}\"  # Assuming sheet names are \"Sheet1\", \"Sheet2\", ..., \"Sheet96\"\n",
    "        df_temp = pd.read_excel(input_file, sheet_name=sheet_name, header=None)  # Read without headers\n",
    "        \n",
    "        # Assign the headers and index from the static data\n",
    "        df_temp.columns = headers\n",
    "        df_temp.index = index\n",
    "        \n",
    "        # Store the DataFrame in the dictionary\n",
    "        df_sheets[t] = df_temp\n",
    "    return df_sheets\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "## Function to update/adjust static OD data for the specific studied line between Nyh and Bål\n",
    "# Groups of stations\n",
    "#C_stations = ['Äs', 'Åbe', 'Sst', 'Cst', 'Ke'] # central stations\n",
    "def update_OD_Nyh_Bal(df):\n",
    "    # Define station groups\n",
    "    R35_stations = ['Nyh', 'Gdv', 'Ngd', 'Öso', 'Ssä', 'Hfa', 'Ts', 'Kda', 'Vhe', 'Jbo', 'Hnd', 'Skg', 'Tåd', 'Fas'] # eastern/right stations of line 35\n",
    "    L35_stations = ['Sub', 'Spå', 'Bkb', 'Jkb', 'Khä', 'Kän', 'Bro', 'Bål'] # western/left stations of line 35\n",
    "    R36_stations = ['Söc', 'Söd', 'Söu', 'Tul', 'Tu', 'Öte', 'Flb', 'Gn', 'Hu', 'Mö', 'Rön', 'Sta'] # eastern/right stations of line 36\n",
    "    L36_stations = ['Sol', 'So', 'Udl', 'Upv', 'U', 'Hel', 'Hgv' , 'Kn', 'Mr', 'Nvk', 'Rs', 'R'] # western/left stations of line 36\n",
    "\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_updated = df.copy()\n",
    "    \n",
    "    # Process trips from 35L to 36L\n",
    "    for origin in L35_stations:\n",
    "        for destination in L36_stations:\n",
    "            df_updated.at[origin, 'Ke'] += df.at[origin, destination]\n",
    "            df_updated.at[origin, destination] = 0\n",
    "    \n",
    "    # Process trips from 35L to 36R\n",
    "    for origin in L35_stations:\n",
    "        for destination in R36_stations:\n",
    "            df_updated.at[origin, 'Äs'] += df.at[origin, destination]\n",
    "            df_updated.at[origin, destination] = 0\n",
    "            \n",
    "    # Process trips from 35R to 36L\n",
    "    for origin in R35_stations:\n",
    "        for destination in L36_stations:\n",
    "            df_updated.at[origin, 'Ke'] += df.at[origin, destination]\n",
    "            df_updated.at[origin, destination] = 0\n",
    "    \n",
    "    # Process trips from 35R to 36R\n",
    "    for origin in R35_stations:\n",
    "        for destination in R36_stations:\n",
    "            df_updated.at[origin, 'Äs'] += df.at[origin, destination]\n",
    "            df_updated.at[origin, destination] = 0\n",
    "    \n",
    "    # Process trips from 36L to 35L\n",
    "    for origin in L36_stations:\n",
    "        for destination in L35_stations:\n",
    "            df_updated.at['Ke', destination] += df.at[origin, destination]\n",
    "            df_updated.at[origin, destination] = 0\n",
    "            \n",
    "    # Process trips from 36L to 35R\n",
    "    for origin in L36_stations:\n",
    "        for destination in R35_stations:\n",
    "            df_updated.at['Äs', destination] += df.at[origin, destination]\n",
    "            df_updated.at[origin, destination] = 0\n",
    "            \n",
    "    # Process trips from 36R to 35L\n",
    "    for origin in R36_stations:\n",
    "        for destination in L35_stations:\n",
    "            df_updated.at['Ke', destination] += df.at[origin, destination]\n",
    "            df_updated.at[origin, destination] = 0\n",
    "    \n",
    "    # Process trips from 36R to 35R\n",
    "    for origin in R36_stations:\n",
    "        for destination in R35_stations:\n",
    "            df_updated.at['Äs', destination] += df.at[origin, destination]\n",
    "            df_updated.at[origin, destination] = 0\n",
    "    \n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and preprocessing of traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read traffic data\n",
    "df_traffic = get_traffic_data()\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = [\n",
    "    \"Tågordning uppdrag\",\n",
    "    \"Tågslag\",\n",
    "    \"Avgångsplats\",\n",
    "    \"Ankomstplats\",\n",
    "    \"Första platssignatur för uppdrag\",\n",
    "    \"Sista platssignatur för uppdrag\",\n",
    "    \"Inställelseorsakskod\",\n",
    "    \"Inställelseorsak\",\n",
    "    \"Dragfordonsid\",\n",
    "    \"Framförda tågkm\",\n",
    "    \"Rapporterad tågvikt\",\n",
    "    \"Rapporterad tåglängd\",\n",
    "    \"Antal rapporterade hjulaxlar\",\n",
    "    \"Antal rapporterade vagnar\",\n",
    "    \"Inställtflagga\"\n",
    "]\n",
    "# drop columns\n",
    "df_traffic_filtered = df_traffic.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# rename the remaining columns, to remove space so that I can use pandas indexing\n",
    "# Dictionary mapping old column names to new column names\n",
    "columns_rename = {\n",
    "    \"Tåguppdrag\": \"Taguppdrag\",\n",
    "    \"Tågnr\": \"Tagnr\",\n",
    "    \"Datum (PAU)\": \"Datum\",\n",
    "    \"UppehållstypAvgång\": \"UppehallstypAvgang\",\n",
    "    \"UppehållstypAnkomst\": \"UppehallstypAnkomst\",\n",
    "    \"Delsträckanummer\": \"Delstrackanummer\",\n",
    "    \"Första platssignatur\": \"Forsta_platssignatur\",\n",
    "    \"Sista platssignatur\": \"Sista_platssignatur\",\n",
    "    \"Från platssignatur\": \"Fran_platssignatur\",\n",
    "    \"Till platssignatur\": \"Till_platssignatur\",\n",
    "    \"Sträcka med riktning\": \"Stracka_med_riktning\",\n",
    "    \"Ankomsttid\": \"Ankomsttid\",\n",
    "    \"Avgångstid\": \"Avgangstid\",\n",
    "    \"Planerad ankomsttid\": \"Planerad_ankomsttid\",\n",
    "    \"Planerad avgångstid\": \"Planerad_avgangstid\",\n",
    "    \"Planeringsstatus\": \"Planeringsstatus\"\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "df_traffic_renamed_all = df_traffic_filtered.rename(columns=columns_rename)\n",
    "\n",
    "df_traffic_renamed_all['Ankomsttid'] = pd.to_datetime(df_traffic_renamed_all['Ankomsttid'], errors='coerce')\n",
    "df_traffic_renamed_all['Avgangstid'] = pd.to_datetime(df_traffic_renamed_all['Avgangstid'], errors='coerce')\n",
    "df_traffic_renamed_all['Planerad_ankomsttid'] = pd.to_datetime(df_traffic_renamed_all['Planerad_ankomsttid'], errors='coerce')\n",
    "df_traffic_renamed_all['Planerad_avgangstid'] = pd.to_datetime(df_traffic_renamed_all['Planerad_avgangstid'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and preprocessing of demand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "\n",
    "# Define valid station values between Nyh and Bål\n",
    "studied_line_35 = ['Nyh', 'Gdv', 'Ngd', 'Öso', 'Ssä', 'Hfa', 'Ts', 'Kda', 'Vhe', 'Jbo', 'Hnd', 'Skg', 'Tåd', 'Fas', 'Äs',\n",
    "                'Åbe', 'Sst', 'Cst', 'Ke', 'Sub', 'Spå', 'Bkb', 'Jkb', 'Khä', 'Kän', 'Bro', 'Bål']\n",
    "\n",
    "# read demand data\n",
    "df_demand = get_demand_data()\n",
    "\n",
    "# Number of time periods (every 15 minutes during a full day)\n",
    "nb_time_periods = int(24 * 60 / 15)  # 96 periods\n",
    "results = []\n",
    "for t in range(nb_time_periods):\n",
    "    # Update the OD matrix\n",
    "    df_temp = update_OD_Nyh_Bal(df_demand[t])\n",
    "    \n",
    "    # Iterate over the updated DataFrame\n",
    "    for origin in studied_line_35:\n",
    "        for destination in studied_line_35:\n",
    "            n_pass = df_temp.at[origin, destination]\n",
    "            results.append({'from': origin, 'to': destination, 'time_period': t, 'n_pass': n_pass})\n",
    "            #results.append([origin, destination, t, n_pass])\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_demand_updated = pd.DataFrame(results)\n",
    "#df_demand_updated_numpy = np.array(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for getting direction and timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get direction\n",
    "# +1 if going north\n",
    "# -1 if going south\n",
    "def get_direction(from_station, to_station, stations_south_to_north):\n",
    "    if stations_south_to_north.index(from_station) < stations_south_to_north.index(to_station):\n",
    "        return 1 # to the north\n",
    "    elif stations_south_to_north.index(from_station) > stations_south_to_north.index(to_station):\n",
    "        return -1 # to the south\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# function returning the index of the time period given a time\n",
    "def get_t_float(time):\n",
    "    return time.hour * 4. + time.minute / 15.\n",
    "def get_t_int(time):\n",
    "    return int(time.hour * 4. + time.minute/15.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove and merge train passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial filtering: Remove rows where both 'UppehallstypAvgang' and 'UppehallstypAnkomst' are 'Passage'\n",
    "df_traffic_renamed = df_traffic_renamed_all[\n",
    "    ~((df_traffic_renamed_all['UppehallstypAvgang'] == 'Passage') & \n",
    "      (df_traffic_renamed_all['UppehallstypAnkomst'] == 'Passage'))\n",
    "]\n",
    "\n",
    "# Identify rows with 'Passage' in either 'UppehallstypAvgang' or 'UppehallstypAnkomst'\n",
    "passage_rows_avg = df_traffic_renamed[df_traffic_renamed['UppehallstypAvgang'] == 'Passage']\n",
    "passage_rows_ank = df_traffic_renamed[df_traffic_renamed['UppehallstypAnkomst'] == 'Passage']\n",
    "\n",
    "# Find the common train groups with 'Passage' in both avgång and ankomst\n",
    "unique_ids = set(zip(passage_rows_avg['Taguppdrag'], passage_rows_avg['Datum'], passage_rows_avg['Tagnr'])) & \\\n",
    "             set(zip(passage_rows_ank['Taguppdrag'], passage_rows_ank['Datum'], passage_rows_ank['Tagnr']))\n",
    "\n",
    "merged_rows = []\n",
    "\n",
    "# Iterate over each unique (Taguppdrag, Datum, Tagnr) tuple\n",
    "for tag_id, datum, tagnr in unique_ids:\n",
    "    # Get the corresponding rows\n",
    "    rows_avg = passage_rows_avg[(passage_rows_avg['Taguppdrag'] == tag_id) & \n",
    "                                (passage_rows_avg['Datum'] == datum) & \n",
    "                                (passage_rows_avg['Tagnr'] == tagnr)].sort_values(by=\"Delstrackanummer\")\n",
    "    \n",
    "    rows_ank = passage_rows_ank[(passage_rows_ank['Taguppdrag'] == tag_id) & \n",
    "                                (passage_rows_ank['Datum'] == datum) & \n",
    "                                (passage_rows_ank['Tagnr'] == tagnr)].sort_values(by=\"Delstrackanummer\")\n",
    "    \n",
    "    # Iterate over each row in the avgång and ankomst sets\n",
    "    for i in range(len(rows_ank)):\n",
    "        # Merge the rows according to the specified rules\n",
    "        merged_row = rows_avg.iloc[i].copy()\n",
    "        merged_row['UppehallstypAvgang'] = rows_ank['UppehallstypAvgang'].iloc[i]\n",
    "        merged_row['UppehallstypAnkomst'] = rows_avg['UppehallstypAnkomst'].iloc[i]\n",
    "        merged_row['Delstrackanummer'] = rows_ank['Delstrackanummer'].iloc[i]\n",
    "        merged_row['Fran_platssignatur'] = rows_ank['Fran_platssignatur'].iloc[i]\n",
    "        merged_row['Till_platssignatur'] = rows_avg['Till_platssignatur'].iloc[i]\n",
    "        merged_row['Ankomsttid'] = rows_avg['Ankomsttid'].iloc[i]\n",
    "        merged_row['Avgangstid'] = rows_ank['Avgangstid'].iloc[i]\n",
    "        merged_row['Planerad_ankomsttid'] = rows_avg['Planerad_ankomsttid'].iloc[i]\n",
    "        merged_row['Planerad_avgangstid'] = rows_ank['Planerad_avgangstid'].iloc[i]\n",
    "\n",
    "        # Add the merged row to the list\n",
    "        merged_rows.append(merged_row)\n",
    "\n",
    "# Create a DataFrame from the merged rows\n",
    "merged_rows_df = pd.DataFrame(merged_rows)\n",
    "\n",
    "# Remove the original 'Passage' rows from the DataFrame\n",
    "traffic_data_filtered = df_traffic_renamed[\n",
    "    ~((df_traffic_renamed['UppehallstypAvgang'] == 'Passage') | \n",
    "      (df_traffic_renamed['UppehallstypAnkomst'] == 'Passage'))\n",
    "]\n",
    "\n",
    "# Add the merged rows back into the DataFrame\n",
    "traffic_data_filtered = pd.concat([traffic_data_filtered, merged_rows_df], ignore_index=True)\n",
    "\n",
    "# Sort by 'Datum', 'Taguppdrag', 'Tagnr', and 'Delstrackanummer'\n",
    "traffic_data_filtered.sort_values(by=['Datum', 'Taguppdrag', 'Tagnr', 'Delstrackanummer'], inplace=True)\n",
    "\n",
    "# Re-adjust 'Delstrackanummer' within each group to be consecutive\n",
    "traffic_data_filtered['Delstrackanummer'] = traffic_data_filtered.groupby(['Datum', 'Taguppdrag', 'Tagnr']).cumcount() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove trains with departures in the day before 2015-09-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the date part only\n",
    "dates_planned = traffic_data_filtered['Planerad_avgangstid'].dt.date\n",
    "\n",
    "# Identify trains departing before 2 am on 2015-09-14\n",
    "early_morning_trains = traffic_data_filtered[\n",
    "    (dates_planned == pd.to_datetime('2015-09-14').date()) & \n",
    "    (traffic_data_filtered['Planerad_avgangstid'].dt.hour < 2)\n",
    "]\n",
    "\n",
    "# Remove the identified trains\n",
    "traffic_data_filtered = traffic_data_filtered.loc[~traffic_data_filtered.index.isin(early_morning_trains.index)]\n",
    "\n",
    "\n",
    "# ## just for testing smaller instance\n",
    "# traffic_data_filtered = traffic_data_filtered[\n",
    "#     ((traffic_data_filtered['Datum'] == '2015-09-14') | \n",
    "#      (traffic_data_filtered['Datum'] == '2015-09-15') |\n",
    "#      (traffic_data_filtered['Datum'] == '2015-09-16') |\n",
    "#      (traffic_data_filtered['Datum'] == '2015-09-17') | \n",
    "#      (traffic_data_filtered['Datum'] == '2015-09-18') | \n",
    "#      (traffic_data_filtered['Datum'] == '2015-09-21'))\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove cancelled trains and correct missing times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correct missing Ankomsttid when Avgangstid exist using Avgangstid and Planerad_avgangstid and Planerad_ankomsttid\n",
    "# Step 1: Filter out only planned (non-cancelled) trains\n",
    "df_traffic_no_cancellation = traffic_data_filtered[traffic_data_filtered[\"Planeringsstatus\"] == \"P\"].copy()\n",
    "\n",
    "# Step 2: Correct missing Ankomsttid using Avgangstid\n",
    "# Filter rows where Ankomsttid is missing but Avgangstid exists\n",
    "missing_arrival_mask = df_traffic_no_cancellation['Ankomsttid'].isna() & df_traffic_no_cancellation['Avgangstid'].notna()\n",
    "\n",
    "# Calculate the difference between Planerad_ankomsttid and Planerad_avgangstid (in minutes)\n",
    "df_traffic_no_cancellation['planned_runtime'] = (df_traffic_no_cancellation['Planerad_ankomsttid'] - df_traffic_no_cancellation['Planerad_avgangstid']).dt.total_seconds() / 60\n",
    "\n",
    "# Calculate the expected Ankomsttid by adding the time difference to Avgangstid\n",
    "df_traffic_no_cancellation.loc[missing_arrival_mask, 'Ankomsttid'] = df_traffic_no_cancellation.loc[missing_arrival_mask, 'Avgangstid'] + pd.to_timedelta(df_traffic_no_cancellation.loc[missing_arrival_mask, 'planned_runtime'], unit='m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set we do the calculations using actual or planned times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_or_planned = 'actual' # or 'actual'\n",
    "if actual_or_planned != 'planned':\n",
    "    # if actual\n",
    "\n",
    "    # Step 1: Group the data by 'Datum', 'Taguppdrag', 'Tagnr'\n",
    "    grouped = df_traffic_no_cancellation.groupby(['Datum', 'Taguppdrag', 'Tagnr'])\n",
    "\n",
    "    # Step 2: Identify groups where any 'Ankomsttid' or 'Avgangstid' is missing\n",
    "    groups_with_missing_values = grouped.filter(lambda group: group[['Ankomsttid', 'Avgangstid']].isnull().any().any())\n",
    "\n",
    "    # Step 3: Remove those groups from the original DataFrame\n",
    "    df_traffic_no_cancellation = df_traffic_no_cancellation[~df_traffic_no_cancellation.index.isin(groups_with_missing_values.index)]\n",
    "\n",
    "    # Step 5: Reset index if needed\n",
    "    df_traffic_no_cancellation = df_traffic_no_cancellation.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate delays for non-cancelled trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## append delay \n",
    "df_traffic_no_cancellation.loc[:,\"dep_delay\"] = df_traffic_no_cancellation[\"Avgangstid\"]-df_traffic_no_cancellation[\"Planerad_avgangstid\"]\n",
    "df_traffic_no_cancellation.loc[:,\"arr_delay\"] = df_traffic_no_cancellation[\"Ankomsttid\"]-df_traffic_no_cancellation[\"Planerad_ankomsttid\"]\n",
    "\n",
    "# Lägg till kolumn för tidsperiod för avgång\n",
    "if actual_or_planned == 'planned':\n",
    "    # if planned\n",
    "    df_traffic_no_cancellation.loc[:,'dep_t_float'] = df_traffic_no_cancellation['Planerad_avgangstid'].apply(get_t_float)\n",
    "    df_traffic_no_cancellation.loc[:,'arr_t_float'] = df_traffic_no_cancellation['Planerad_ankomsttid'].apply(get_t_float)\n",
    "else:\n",
    "    # if actual\n",
    "    df_traffic_no_cancellation.loc[:,'dep_t_float'] = df_traffic_no_cancellation['Avgangstid'].apply(get_t_float)\n",
    "    df_traffic_no_cancellation.loc[:,'arr_t_float'] = df_traffic_no_cancellation['Ankomsttid'].apply(get_t_float)\n",
    "\n",
    "# Function to convert timedelta to minutes, negative values and NaT to zero\n",
    "def timedelta_to_minutes(td):\n",
    "    if pd.isna(td) or td.total_seconds() < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return td.total_seconds() / 60\n",
    "\n",
    "# Apply the function to 'dep_delay' and 'arr_delay' columns\n",
    "df_traffic_no_cancellation.loc[:,'dep_delay_minutes'] = df_traffic_no_cancellation['dep_delay'].apply(timedelta_to_minutes)\n",
    "df_traffic_no_cancellation.loc[:,'arr_delay_minutes'] = df_traffic_no_cancellation['arr_delay'].apply(timedelta_to_minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataframes into north and south-going trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of stations from south to north\n",
    "stations_south_to_north = ['Bål', 'Bro', 'Kän', 'Khä', 'Jkb', 'Bkb', 'Spå', 'Sub', 'Ke', 'Cst', 'Sst', 'Åbe', 'Äs',\n",
    "                           'Fas', 'Tåd', 'Skg', 'Hnd', 'Jbo', 'Vhe', 'Kda', 'Ts', 'Hfa', 'Ssä', 'Öso', 'Ngd', 'Gdv', 'Nyh']\n",
    "\n",
    "# Determine direction and split the data\n",
    "df_traffic_no_cancellation.loc[:,'direction'] = df_traffic_no_cancellation.apply(lambda row: get_direction(row['Fran_platssignatur'], row['Till_platssignatur'], stations_south_to_north), axis=1)\n",
    "df_to_north = df_traffic_no_cancellation[df_traffic_no_cancellation['direction'] == 1].copy()\n",
    "df_to_south = df_traffic_no_cancellation[df_traffic_no_cancellation['direction'] == -1].copy()\n",
    "\n",
    "# Step 1: Process for each dataframe (e.g., df_to_north)\n",
    "def process_directional_dataframe(df):\n",
    "    # Group by departure station and sort by planned departure time\n",
    "    \n",
    "    # if planned \n",
    "    if actual_or_planned == 'planned':\n",
    "    # if planned\n",
    "        df_sorted = df.sort_values(by=['Fran_platssignatur', 'Planerad_avgangstid'])\n",
    "    else:\n",
    "        df_sorted = df.sort_values(by=['Fran_platssignatur', 'Avgangstid'])\n",
    "\n",
    "    # Calculate previous time periods\n",
    "    df_sorted['prev_dep_t_float'] = df_sorted.groupby('Fran_platssignatur')['dep_t_float'].shift(1, fill_value=0)\n",
    "\n",
    "    # the first train of the day has prev in the day before and (prev_t>curr_t)\n",
    "    # also if you remove trains flagged with A or I, you won't get prev_t = curr_t\n",
    "    df_sorted.loc[df_sorted['prev_dep_t_float'] > df_sorted['dep_t_float'],'prev_dep_t_float'] = 0. \n",
    "    \n",
    "    return df_sorted.copy()\n",
    "\n",
    "df_to_north = process_directional_dataframe(df_to_north)\n",
    "df_to_south = process_directional_dataframe(df_to_south)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate with boarding passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_boarding_vectorized_all_stations():\n",
    "    # This function returns a DataFrame with total passengers boarding at each station between different time periods for each destination\n",
    "    boarding_data = df_demand_updated.copy()\n",
    "\n",
    "    # Group by 'from', 'time_period' and 'to' and sum 'n_pass' for each destination\n",
    "    boarding_sum = (boarding_data.groupby(['from', 'time_period', 'to'])['n_pass']\n",
    "                                   .sum()\n",
    "                                   .unstack(fill_value=0)\n",
    "                                   .reindex(columns=studied_line_35, fill_value=0))\n",
    "\n",
    "    return boarding_sum\n",
    "\n",
    "def get_n_boarding_float(data_boarding, t_prev, t_curr):\n",
    "    res_boardings = data_boarding.loc[int(t_prev):int(t_curr)].sum()\n",
    "    res_boardings = res_boardings - data_boarding.loc[int(t_prev)]*(t_prev-int(t_prev))\n",
    "    res_boardings = res_boardings - data_boarding.loc[int(t_curr)]*(int(t_curr)+1-t_curr)\n",
    "    return res_boardings\n",
    "\n",
    "# Step 2: Calculate passengers boarding\n",
    "def calculate_boarding_passengers(df, boarding_sum):\n",
    "    # this function considers the direction of travel\n",
    "\n",
    "    # Initialize new columns for boardings with zeros\n",
    "    df[studied_line_35] = 0.0\n",
    "\n",
    "    # Group the data by 'Fran_platssignatur'\n",
    "    grouped = df.groupby('Fran_platssignatur')\n",
    "\n",
    "    # For each group, perform the vectorized calculation\n",
    "    for from_station, group in grouped:\n",
    "        # Get the previous and current time periods\n",
    "        prev_t = group['prev_dep_t_float'].values # use float instead\n",
    "        curr_t = group['dep_t_float'].values\n",
    "\n",
    "        # Compute the cumulative boarding for each destination over the period\n",
    "        for i, index in enumerate(group.index):\n",
    "            n_boarding = get_n_boarding_float(boarding_sum.loc[from_station], prev_t[i], curr_t[i])\n",
    "            df.loc[index, studied_line_35] = n_boarding[studied_line_35]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Precompute the boarding sums for all stations\n",
    "boarding_sum = get_n_boarding_vectorized_all_stations()\n",
    "\n",
    "# Apply the calculation to the northbound and southbound dataframes\n",
    "#df_to_north_boardings = calculate_boarding_passengers(df_to_north, boarding_sum)\n",
    "# df_to_south_boardings = calculate_boarding_passengers(df_to_south, boarding_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Parallel execution\n",
    "results = Parallel(n_jobs=2)(\n",
    "    delayed(calculate_boarding_passengers)(df, boarding_sum)\n",
    "    for df in [df_to_north, df_to_south]\n",
    ")\n",
    "\n",
    "# Unpack the results\n",
    "df_to_north_boardings, df_to_south_boardings = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculte and populate with passengers onboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_onboard_passengers(df):\n",
    "    # Step 3: calculate the passengers onboard using the boarding passengers\n",
    "    #df['n_onboard'] = 0.0  # Initialize the onboard column\n",
    "    \n",
    "    def calculate_onboard(group):\n",
    "        # Sort by 'Delstrackanummer' to get the correct order of stops\n",
    "        group_sorted = group.sort_values(by='Delstrackanummer').copy()\n",
    "        \n",
    "        # Initialize the onboard count\n",
    "        group_sorted['n_onboard'] = 0.0\n",
    "        \n",
    "        for i, index in enumerate(group_sorted.index):\n",
    "            # Current station data\n",
    "            curr_row = group_sorted.iloc[i]\n",
    "            next_stations = group_sorted.iloc[i:]['Till_platssignatur'].tolist()\n",
    "\n",
    "            # Total passengers boarding at this stop to any future stop\n",
    "            total_boarding = curr_row[next_stations].sum()\n",
    "            group_sorted.loc[index, 'n_boarding'] = total_boarding\n",
    "            # Create a boolean array indicating which stations are not in the next_stations list\n",
    "            not_next_stations = ~pd.Series(studied_line_35).isin(next_stations)\n",
    "            # Set the values to 0 for the stations not in the next_stations list\n",
    "            group_sorted.loc[index, pd.Series(studied_line_35)[not_next_stations]] = 0\n",
    "            if i == 0:\n",
    "                # First stop, no previous passengers onboard\n",
    "                group_sorted.loc[index, 'n_onboard'] = total_boarding\n",
    "                group_sorted.loc[index, 'n_alighting'] = 0. \n",
    "            else:\n",
    "                # Passengers onboard from the previous stop\n",
    "                prev_onboard = group_sorted.iloc[i-1]['n_onboard']\n",
    "                \n",
    "                # Passengers alighting at this stop: boarding from previous stops to this one\n",
    "                n_alighting = group_sorted.iloc[:i][curr_row['Fran_platssignatur']].sum()\n",
    "                group_sorted.loc[index, 'n_alighting'] =n_alighting\n",
    "\n",
    "                # Update the onboard count\n",
    "                n_onboard = prev_onboard + total_boarding - n_alighting\n",
    "                group_sorted.loc[index, 'n_onboard'] = n_onboard\n",
    " \n",
    "            # save the alighting in the next station\n",
    "            n_alighting_next = group_sorted.iloc[:i+1][curr_row['Till_platssignatur']].sum()\n",
    "            group_sorted.loc[index, 'n_alighting_next'] = n_alighting_next\n",
    "\n",
    "        return group_sorted\n",
    "    \n",
    "    # Apply the calculation for each train and date combination\n",
    "    df_res = df.groupby(['Taguppdrag', 'Tagnr', 'Datum']).apply(calculate_onboard, include_groups=False)\n",
    "    df_res = df_res.reset_index().drop(columns=['level_3'])\n",
    "    \n",
    "    return df_res\n",
    "\n",
    "# Apply the calculation to the northbound and southbound dataframes\n",
    "#df_to_north_onboard = calculate_onboard_passengers(df_to_north_boardings)\n",
    "# df_to_south_onboard = calculate_onboard_passengers(df_to_south_boardings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel execution\n",
    "results = Parallel(n_jobs=2)(\n",
    "    delayed(calculate_onboard_passengers)(df)\n",
    "    for df in [df_to_north_boardings, df_to_south_boardings]\n",
    ")\n",
    "\n",
    "# Unpack the results\n",
    "df_to_north_onboard, df_to_south_onboard = results\n",
    "\n",
    "\n",
    "# Combine northbound and southbound dataframes\n",
    "df_onboard_concat = pd.concat([df_to_north_onboard, df_to_south_onboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index if necessary\n",
    "df_onboard = df_onboard_concat.reset_index(drop=True)#.drop(columns=studied_line_35, axis=1)\n",
    "if actual_or_planned == 'planned':\n",
    "# if planned\n",
    "    df_onboard.to_csv('df_onboard_planned.csv', index=False)  \n",
    "else:\n",
    "    df_onboard.to_csv('df_onboard_actual.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
